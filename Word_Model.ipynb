{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Seq2Seq Language Translation model for any language pair of your choice. (See Datasets: http://www.manythings.org/anki/ )\n",
    "\n",
    "(1) Build a character-level model \n",
    "\n",
    "(2) Build a word level model\n",
    "\n",
    "(3) Build a word model with attention\n",
    "\n",
    "Submit code and pdf. Include sample inferences from best performing model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word level model\n",
    "#### Dataset used : French Language (fra.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.python.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "line= pd.read_table('fra.txt', nrows=10000, usecols=[0,1], names=['eng', 'fre'])\n",
    "\n",
    "line.eng=line.eng.apply(lambda x: x.lower())\n",
    "line.fre=line.fre.apply(lambda x: x.lower())\n",
    "\n",
    "line.eng=line.eng.apply(lambda x: re.sub(\"'\", '', str(x)))\n",
    "line.fre=line.fre.apply(lambda x: re.sub(\"'\", '', str(x)))\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "line.eng=line.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "line.fre=line.fre.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "line.fre = line.fre.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "line.eng=line.eng.apply(lambda x: x.translate(remove_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4568"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in line.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "            \n",
    "all_french_words=set() # will have only unique words of French\n",
    "for fre in line.fre:\n",
    "    for word in fre.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in line.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_src = np.max(lenght_list)\n",
    "max_length_src\n",
    "\n",
    "#Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in line.fre:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_tar = np.max(lenght_list)\n",
    "max_length_tar\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ va  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_ salut  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_ salut _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>run</td>\n",
       "      <td>START_ cours  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>run</td>\n",
       "      <td>START_ courez  _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng                  fre\n",
       "0   go      START_ va  _END\n",
       "1   hi   START_ salut  _END\n",
       "2   hi    START_ salut _END\n",
       "3  run   START_ cours  _END\n",
       "4  run  START_ courez  _END"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (2000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test data\n",
    "X, Y = line.eng, line.fre\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Zero arrays\n",
    "def generate_batch(X = X_train, Y = Y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], Y[j:j+batch_size])):  \n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the word level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens+1, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard 'encoder_outputs' and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens+1, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences, and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_target_data'\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     107900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     228450      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50), (None,  20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4568)   232968      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 609,718\n",
      "Trainable params: 609,718\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Compile the Model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-016e239dcd84>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 2.3572 - acc: 0.2331 - val_loss: 1.9308 - val_acc: 0.2362\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 1.8557 - acc: 0.2378 - val_loss: 1.9105 - val_acc: 0.2362\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 33s 540ms/step - loss: 1.8238 - acc: 0.2377 - val_loss: 1.9135 - val_acc: 0.2362\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 29s 463ms/step - loss: 1.7928 - acc: 0.2451 - val_loss: 1.8899 - val_acc: 0.2703\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 26s 417ms/step - loss: 1.7249 - acc: 0.2800 - val_loss: 1.7968 - val_acc: 0.2832\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 25s 408ms/step - loss: 1.6347 - acc: 0.2946 - val_loss: 1.7235 - val_acc: 0.3025\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 1.5701 - acc: 0.3081 - val_loss: 1.6782 - val_acc: 0.3084\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 1.5201 - acc: 0.3136 - val_loss: 1.6435 - val_acc: 0.3167\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 1.4782 - acc: 0.3267 - val_loss: 1.6120 - val_acc: 0.3273\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 24s 382ms/step - loss: 1.4402 - acc: 0.3357 - val_loss: 1.5804 - val_acc: 0.3361\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 23s 379ms/step - loss: 1.4063 - acc: 0.3485 - val_loss: 1.5526 - val_acc: 0.3554\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 23s 377ms/step - loss: 1.3707 - acc: 0.3659 - val_loss: 1.5246 - val_acc: 0.3688\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 23s 379ms/step - loss: 1.3347 - acc: 0.3770 - val_loss: 1.5003 - val_acc: 0.3777\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 24s 382ms/step - loss: 1.3025 - acc: 0.3966 - val_loss: 1.4733 - val_acc: 0.4007\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 23s 377ms/step - loss: 1.2692 - acc: 0.4121 - val_loss: 1.4517 - val_acc: 0.4118\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 24s 383ms/step - loss: 1.2407 - acc: 0.4234 - val_loss: 1.4324 - val_acc: 0.4209\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 24s 380ms/step - loss: 1.2167 - acc: 0.4335 - val_loss: 1.4161 - val_acc: 0.4248\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 23s 379ms/step - loss: 1.1911 - acc: 0.4414 - val_loss: 1.3989 - val_acc: 0.4327\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 25s 404ms/step - loss: 1.1674 - acc: 0.4501 - val_loss: 1.3841 - val_acc: 0.4416\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 24s 385ms/step - loss: 1.1436 - acc: 0.4592 - val_loss: 1.3684 - val_acc: 0.4496\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 23s 379ms/step - loss: 1.1223 - acc: 0.4662 - val_loss: 1.3550 - val_acc: 0.4531\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 23s 377ms/step - loss: 1.1027 - acc: 0.4734 - val_loss: 1.3432 - val_acc: 0.4596\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 23s 375ms/step - loss: 1.0808 - acc: 0.4807 - val_loss: 1.3336 - val_acc: 0.4599\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 25s 401ms/step - loss: 1.0622 - acc: 0.4867 - val_loss: 1.3217 - val_acc: 0.4665\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 24s 379ms/step - loss: 1.0450 - acc: 0.4922 - val_loss: 1.3115 - val_acc: 0.4676\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 24s 380ms/step - loss: 1.0279 - acc: 0.4983 - val_loss: 1.3021 - val_acc: 0.4713\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 24s 382ms/step - loss: 1.0114 - acc: 0.5035 - val_loss: 1.2950 - val_acc: 0.4740\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 24s 385ms/step - loss: 0.9938 - acc: 0.5082 - val_loss: 1.2860 - val_acc: 0.4737\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 24s 393ms/step - loss: 0.9805 - acc: 0.5127 - val_loss: 1.2803 - val_acc: 0.4783\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 24s 383ms/step - loss: 0.9643 - acc: 0.5167 - val_loss: 1.2730 - val_acc: 0.4780\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 25s 403ms/step - loss: 0.9505 - acc: 0.5209 - val_loss: 1.2645 - val_acc: 0.4797\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 0.9343 - acc: 0.5254 - val_loss: 1.2596 - val_acc: 0.4808\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 25s 410ms/step - loss: 0.9232 - acc: 0.5288 - val_loss: 1.2529 - val_acc: 0.4817\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.9092 - acc: 0.5329 - val_loss: 1.2487 - val_acc: 0.4819\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 24s 383ms/step - loss: 0.8961 - acc: 0.5372 - val_loss: 1.2467 - val_acc: 0.4834\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 24s 381ms/step - loss: 0.8842 - acc: 0.5415 - val_loss: 1.2427 - val_acc: 0.4849\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 24s 382ms/step - loss: 0.8720 - acc: 0.5458 - val_loss: 1.2351 - val_acc: 0.4865\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 24s 386ms/step - loss: 0.8591 - acc: 0.5495 - val_loss: 1.2287 - val_acc: 0.4915\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 24s 381ms/step - loss: 0.8485 - acc: 0.5541 - val_loss: 1.2289 - val_acc: 0.4904\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 24s 382ms/step - loss: 0.8368 - acc: 0.5584 - val_loss: 1.2226 - val_acc: 0.4911\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 24s 386ms/step - loss: 0.8257 - acc: 0.5631 - val_loss: 1.2184 - val_acc: 0.4941\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 24s 383ms/step - loss: 0.8153 - acc: 0.5662 - val_loss: 1.2156 - val_acc: 0.4938\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 24s 389ms/step - loss: 0.8046 - acc: 0.5702 - val_loss: 1.2149 - val_acc: 0.4930\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 24s 394ms/step - loss: 0.7950 - acc: 0.5734 - val_loss: 1.2099 - val_acc: 0.4973\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 24s 394ms/step - loss: 0.7848 - acc: 0.5781 - val_loss: 1.2065 - val_acc: 0.4962\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 25s 408ms/step - loss: 0.7749 - acc: 0.5813 - val_loss: 1.2018 - val_acc: 0.4991\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 24s 392ms/step - loss: 0.7651 - acc: 0.5851 - val_loss: 1.1998 - val_acc: 0.4995\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 24s 394ms/step - loss: 0.7566 - acc: 0.5880 - val_loss: 1.1975 - val_acc: 0.4986\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 24s 389ms/step - loss: 0.7465 - acc: 0.5911 - val_loss: 1.1927 - val_acc: 0.5052\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.7369 - acc: 0.5952 - val_loss: 1.1937 - val_acc: 0.5027\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.7278 - acc: 0.5987 - val_loss: 1.1935 - val_acc: 0.5043\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 24s 392ms/step - loss: 0.7178 - acc: 0.6015 - val_loss: 1.1928 - val_acc: 0.5034\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 0.7084 - acc: 0.6055 - val_loss: 1.1911 - val_acc: 0.5032\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.6993 - acc: 0.6079 - val_loss: 1.1863 - val_acc: 0.5053\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 24s 386ms/step - loss: 0.6908 - acc: 0.6108 - val_loss: 1.1834 - val_acc: 0.5079\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 25s 396ms/step - loss: 0.6809 - acc: 0.6141 - val_loss: 1.1804 - val_acc: 0.5085\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 25s 399ms/step - loss: 0.6725 - acc: 0.6181 - val_loss: 1.1712 - val_acc: 0.5113\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 25s 400ms/step - loss: 0.6635 - acc: 0.6209 - val_loss: 1.1851 - val_acc: 0.5054\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 24s 389ms/step - loss: 0.6561 - acc: 0.6235 - val_loss: 1.1750 - val_acc: 0.5097\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 24s 389ms/step - loss: 0.6475 - acc: 0.6273 - val_loss: 1.1707 - val_acc: 0.5139\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 0.6395 - acc: 0.6302 - val_loss: 1.1694 - val_acc: 0.5132\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 24s 387ms/step - loss: 0.6317 - acc: 0.6338 - val_loss: 1.1674 - val_acc: 0.5138\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.6247 - acc: 0.6368 - val_loss: 1.1642 - val_acc: 0.5176\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 25s 409ms/step - loss: 0.6222 - acc: 0.6403 - val_loss: 1.1665 - val_acc: 0.5150\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.6112 - acc: 0.6421 - val_loss: 1.1659 - val_acc: 0.5177\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 26s 414ms/step - loss: 0.6032 - acc: 0.6456 - val_loss: 1.1653 - val_acc: 0.5161\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.5980 - acc: 0.6480 - val_loss: 1.1616 - val_acc: 0.5173\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 26s 413ms/step - loss: 0.5918 - acc: 0.6512 - val_loss: 1.1603 - val_acc: 0.5181\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 0.5862 - acc: 0.6543 - val_loss: 1.1589 - val_acc: 0.5201\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 24s 384ms/step - loss: 0.5798 - acc: 0.6571 - val_loss: 1.1582 - val_acc: 0.5210\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 24s 384ms/step - loss: 0.5737 - acc: 0.6609 - val_loss: 1.1622 - val_acc: 0.5180\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 24s 387ms/step - loss: 0.5689 - acc: 0.6627 - val_loss: 1.1610 - val_acc: 0.5207\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 24s 385ms/step - loss: 0.5624 - acc: 0.6648 - val_loss: 1.1618 - val_acc: 0.5180\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.5573 - acc: 0.6680 - val_loss: 1.1620 - val_acc: 0.5172\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.5512 - acc: 0.6698 - val_loss: 1.1570 - val_acc: 0.5221\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 24s 387ms/step - loss: 0.5456 - acc: 0.6731 - val_loss: 1.1650 - val_acc: 0.5175\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.5407 - acc: 0.6753 - val_loss: 1.1604 - val_acc: 0.5191\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 24s 387ms/step - loss: 0.5348 - acc: 0.6772 - val_loss: 1.1565 - val_acc: 0.5219\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 24s 386ms/step - loss: 0.5287 - acc: 0.6814 - val_loss: 1.1575 - val_acc: 0.5225\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.5250 - acc: 0.6828 - val_loss: 1.1581 - val_acc: 0.5224\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 25s 406ms/step - loss: 0.5201 - acc: 0.6853 - val_loss: 1.1579 - val_acc: 0.5228\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.5147 - acc: 0.6879 - val_loss: 1.1651 - val_acc: 0.5196\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.5103 - acc: 0.6900 - val_loss: 1.1639 - val_acc: 0.5214\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 0.5054 - acc: 0.6939 - val_loss: 1.1558 - val_acc: 0.5240\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 24s 390ms/step - loss: 0.5004 - acc: 0.6958 - val_loss: 1.1610 - val_acc: 0.5236\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 24s 392ms/step - loss: 0.4943 - acc: 0.6987 - val_loss: 1.1551 - val_acc: 0.5252\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.4901 - acc: 0.6999 - val_loss: 1.1514 - val_acc: 0.5279\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 24s 388ms/step - loss: 0.4853 - acc: 0.7018 - val_loss: 1.1514 - val_acc: 0.5269\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 24s 384ms/step - loss: 0.4804 - acc: 0.7057 - val_loss: 1.1501 - val_acc: 0.5285\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 24s 386ms/step - loss: 0.4761 - acc: 0.7076 - val_loss: 1.1552 - val_acc: 0.5255\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 24s 384ms/step - loss: 0.4704 - acc: 0.7113 - val_loss: 1.1570 - val_acc: 0.5265\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 25s 401ms/step - loss: 0.4676 - acc: 0.7129 - val_loss: 1.1525 - val_acc: 0.5279\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 25s 397ms/step - loss: 0.4623 - acc: 0.7146 - val_loss: 1.1540 - val_acc: 0.5278\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 25s 403ms/step - loss: 0.4581 - acc: 0.7183 - val_loss: 1.1605 - val_acc: 0.5274\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 24s 387ms/step - loss: 0.4531 - acc: 0.7204 - val_loss: 1.1573 - val_acc: 0.5274\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.4498 - acc: 0.7224 - val_loss: 1.1638 - val_acc: 0.5256\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.4450 - acc: 0.7250 - val_loss: 1.1648 - val_acc: 0.5236\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.4407 - acc: 0.7271 - val_loss: 1.1670 - val_acc: 0.5245\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 24s 392ms/step - loss: 0.4372 - acc: 0.7296 - val_loss: 1.1641 - val_acc: 0.5256\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 24s 391ms/step - loss: 0.4332 - acc: 0.7312 - val_loss: 1.1607 - val_acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28e3cfff648>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Model\n",
    "model.fit_generator(generator = generate_batch(X_train, Y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, Y_test, batch_size =batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Predictions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, Y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Source sentence: youre in luck\n",
      "Actual Target Translation:  vous êtes veinards \n",
      "Predicted Target Translation:  vous êtes veinard \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Target Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Target Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: use them all\n",
      "Actual French Translation:  utilisezles toutes  \n",
      "Predicted French Translation:  utilisezles toutes \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual French Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted French Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: once again\n",
      "Actual French Translation:  encore une fois \n",
      "Predicted French Translation:  encore une fois \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual French Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted French Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "* The Seq2seq Model built using the Word-based model gives a Validation Accuracy of 73%.\n",
    "* The Accuracy of this Model is much higher in comparison to the character-based seq2seq Translation Model, which only gave an accuracy of 22%.\n",
    "* The word-based model is accurately predicting single words. \n",
    "* It also seems to be performing quite well even on successive words in small sentences, as can be seen in the above sample predictions.\n",
    "\n",
    "=> Thus, based on these observations we can infer that the word-based model is more reliable and efficient for Language Translation features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
