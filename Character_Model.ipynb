{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seq2Seq Language Translation model\n",
    "##### Dataset used : French Language (fra.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build a Seq2Seq Language Translation model for any language pair of your choice. (See Datasets: http://www.manythings.org/anki/ )\n",
    "\n",
    "(1) Build a character-level model \n",
    "\n",
    "(2) Build a word level model\n",
    "\n",
    "(3) Build a word model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Character level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.python.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the inputs and encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len= 10000\n",
    "df = 'fra.txt'\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "ip_chars = set()\n",
    "op_chars = set()\n",
    "with open(df, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "    \n",
    "#Inputs have two parts, the English text ie input_text and input target ie target_text    \n",
    "for line in lines[: min(sample_len, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')[:2]\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in ip_chars:\n",
    "            ip_chars.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in op_chars:\n",
    "            op_chars.add(char)\n",
    "\n",
    "ip_chars = sorted(list(ip_chars))\n",
    "op_chars = sorted(list(op_chars))\n",
    "num_encoder_tokens = len(ip_chars)\n",
    "num_decoder_tokens = len(op_chars)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.',\n",
       " 'Hi.',\n",
       " 'Hi.',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Who?',\n",
       " 'Wow!',\n",
       " 'Fire!',\n",
       " 'Help!',\n",
       " 'Jump.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tVa !\\n',\n",
       " '\\tSalut !\\n',\n",
       " '\\tSalut.\\n',\n",
       " '\\tCours\\u202f!\\n',\n",
       " '\\tCourez\\u202f!\\n',\n",
       " '\\tQui ?\\n',\n",
       " '\\tÇa alors\\u202f!\\n',\n",
       " '\\tAu feu !\\n',\n",
       " \"\\tÀ l'aide\\u202f!\\n\",\n",
       " '\\tSaute.\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_tokens = dict([(char, i) for i, char in enumerate(ip_chars)])\n",
    "op_tokens = dict([(char, i) for i, char in enumerate(op_chars)])\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, ip_tokens[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, op_tokens[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, op_tokens[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the character-level model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "epochs = 100\n",
    "latent_dim = 256 \n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 256), (None, 335872      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, None, 256),  358400      input_22[0][0]                   \n",
      "                                                                 lstm_10[0][1]                    \n",
      "                                                                 lstm_10[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 93)     23901       lstm_11[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55/55 [==============================] - 100s 2s/step - loss: 1.0271 - acc: 0.0478 - val_loss: 1.0175 - val_acc: 0.0631\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.8766 - acc: 0.0656 - val_loss: 0.9705 - val_acc: 0.0718\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 141s 3s/step - loss: 0.8076 - acc: 0.0888 - val_loss: 0.8632 - val_acc: 0.1146\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 141s 3s/step - loss: 0.7131 - acc: 0.1159 - val_loss: 0.7794 - val_acc: 0.1333\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 143s 3s/step - loss: 0.6422 - acc: 0.1324 - val_loss: 0.7214 - val_acc: 0.1450\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 144s 3s/step - loss: 0.5960 - acc: 0.1419 - val_loss: 0.6801 - val_acc: 0.1537\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 146s 3s/step - loss: 0.5643 - acc: 0.1491 - val_loss: 0.6522 - val_acc: 0.1613\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 147s 3s/step - loss: 0.5402 - acc: 0.1550 - val_loss: 0.6301 - val_acc: 0.1648\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 146s 3s/step - loss: 0.5214 - acc: 0.1601 - val_loss: 0.6139 - val_acc: 0.1724\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.5033 - acc: 0.1653 - val_loss: 0.5955 - val_acc: 0.1775\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.4890 - acc: 0.1689 - val_loss: 0.5777 - val_acc: 0.1828\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.4756 - acc: 0.1720 - val_loss: 0.5711 - val_acc: 0.1830\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 146s 3s/step - loss: 0.4639 - acc: 0.1754 - val_loss: 0.5560 - val_acc: 0.1883\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 146s 3s/step - loss: 0.4517 - acc: 0.1785 - val_loss: 0.5476 - val_acc: 0.1901\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.4413 - acc: 0.1817 - val_loss: 0.5374 - val_acc: 0.1940\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 147s 3s/step - loss: 0.4320 - acc: 0.1842 - val_loss: 0.5319 - val_acc: 0.1956\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 150s 3s/step - loss: 0.4239 - acc: 0.1865 - val_loss: 0.5260 - val_acc: 0.1965\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 146s 3s/step - loss: 0.4139 - acc: 0.1889 - val_loss: 0.5214 - val_acc: 0.1978\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 141s 3s/step - loss: 0.4049 - acc: 0.1916 - val_loss: 0.5119 - val_acc: 0.2004\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.3968 - acc: 0.1938 - val_loss: 0.5065 - val_acc: 0.2024\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 150s 3s/step - loss: 0.3900 - acc: 0.1959 - val_loss: 0.5040 - val_acc: 0.2032\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 158s 3s/step - loss: 0.3822 - acc: 0.1983 - val_loss: 0.5000 - val_acc: 0.2034\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.3756 - acc: 0.1999 - val_loss: 0.4934 - val_acc: 0.2060\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 140s 3s/step - loss: 0.3683 - acc: 0.2020 - val_loss: 0.4909 - val_acc: 0.2076\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 164s 3s/step - loss: 0.3614 - acc: 0.2042 - val_loss: 0.4849 - val_acc: 0.2091\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 165s 3s/step - loss: 0.3539 - acc: 0.2064 - val_loss: 0.4821 - val_acc: 0.2095\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 155s 3s/step - loss: 0.3478 - acc: 0.2082 - val_loss: 0.4824 - val_acc: 0.2088\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 196s 4s/step - loss: 0.3413 - acc: 0.2101 - val_loss: 0.4756 - val_acc: 0.2114\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 191s 3s/step - loss: 0.3358 - acc: 0.2118 - val_loss: 0.4712 - val_acc: 0.2136\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 181s 3s/step - loss: 0.3287 - acc: 0.2137 - val_loss: 0.4701 - val_acc: 0.2129\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.3232 - acc: 0.2154 - val_loss: 0.4730 - val_acc: 0.2125\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 148s 3s/step - loss: 0.3176 - acc: 0.2174 - val_loss: 0.4673 - val_acc: 0.2152\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 148s 3s/step - loss: 0.3115 - acc: 0.2191 - val_loss: 0.4648 - val_acc: 0.2161\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 148s 3s/step - loss: 0.3061 - acc: 0.2207 - val_loss: 0.4629 - val_acc: 0.2160\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.3004 - acc: 0.2222 - val_loss: 0.4598 - val_acc: 0.2175\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 198s 4s/step - loss: 0.2967 - acc: 0.2236 - val_loss: 0.4638 - val_acc: 0.2167\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 178s 3s/step - loss: 0.2904 - acc: 0.2252 - val_loss: 0.4617 - val_acc: 0.2176\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 160s 3s/step - loss: 0.2854 - acc: 0.2268 - val_loss: 0.4573 - val_acc: 0.2195\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 156s 3s/step - loss: 0.2796 - acc: 0.2286 - val_loss: 0.4587 - val_acc: 0.2192\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.2757 - acc: 0.2298 - val_loss: 0.4562 - val_acc: 0.2194\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 160s 3s/step - loss: 0.2708 - acc: 0.2309 - val_loss: 0.4563 - val_acc: 0.2200\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 160s 3s/step - loss: 0.2661 - acc: 0.2327 - val_loss: 0.4554 - val_acc: 0.2206\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 156s 3s/step - loss: 0.2607 - acc: 0.2341 - val_loss: 0.4573 - val_acc: 0.2210\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 161s 3s/step - loss: 0.2554 - acc: 0.2357 - val_loss: 0.4577 - val_acc: 0.2210\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 160s 3s/step - loss: 0.2512 - acc: 0.2367 - val_loss: 0.4572 - val_acc: 0.2219\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 147s 3s/step - loss: 0.2486 - acc: 0.2377 - val_loss: 0.4596 - val_acc: 0.2211\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.2439 - acc: 0.2390 - val_loss: 0.4577 - val_acc: 0.2220\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 150s 3s/step - loss: 0.2385 - acc: 0.2409 - val_loss: 0.4623 - val_acc: 0.2218\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 148s 3s/step - loss: 0.2349 - acc: 0.2420 - val_loss: 0.4625 - val_acc: 0.2219\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.2305 - acc: 0.2430 - val_loss: 0.4634 - val_acc: 0.2229\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.2261 - acc: 0.2446 - val_loss: 0.4629 - val_acc: 0.2232\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.2222 - acc: 0.2456 - val_loss: 0.4672 - val_acc: 0.2223\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 159s 3s/step - loss: 0.2178 - acc: 0.2469 - val_loss: 0.4664 - val_acc: 0.2225\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 156s 3s/step - loss: 0.2134 - acc: 0.2480 - val_loss: 0.4669 - val_acc: 0.2233\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 156s 3s/step - loss: 0.2106 - acc: 0.2490 - val_loss: 0.4729 - val_acc: 0.2227\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 148s 3s/step - loss: 0.2074 - acc: 0.2500 - val_loss: 0.4742 - val_acc: 0.2233\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.2027 - acc: 0.2517 - val_loss: 0.4759 - val_acc: 0.2228\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1989 - acc: 0.2526 - val_loss: 0.4769 - val_acc: 0.2230\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1958 - acc: 0.2534 - val_loss: 0.4768 - val_acc: 0.2235\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1915 - acc: 0.2547 - val_loss: 0.4839 - val_acc: 0.2228\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1891 - acc: 0.2555 - val_loss: 0.4828 - val_acc: 0.2232\n",
      "Epoch 62/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1856 - acc: 0.2566 - val_loss: 0.4854 - val_acc: 0.2235\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1816 - acc: 0.2578 - val_loss: 0.4877 - val_acc: 0.2228\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 149s 3s/step - loss: 0.1793 - acc: 0.2585 - val_loss: 0.4932 - val_acc: 0.2226\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1758 - acc: 0.2594 - val_loss: 0.4927 - val_acc: 0.2235\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1728 - acc: 0.2603 - val_loss: 0.4958 - val_acc: 0.2227\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1695 - acc: 0.2615 - val_loss: 0.4976 - val_acc: 0.2230\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 155s 3s/step - loss: 0.1667 - acc: 0.2623 - val_loss: 0.5011 - val_acc: 0.2225\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1640 - acc: 0.2629 - val_loss: 0.5028 - val_acc: 0.2227\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1609 - acc: 0.2640 - val_loss: 0.5050 - val_acc: 0.2229\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1575 - acc: 0.2650 - val_loss: 0.5098 - val_acc: 0.2225\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1550 - acc: 0.2658 - val_loss: 0.5133 - val_acc: 0.2231\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1516 - acc: 0.2670 - val_loss: 0.5149 - val_acc: 0.2234\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1523 - acc: 0.2661 - val_loss: 0.5190 - val_acc: 0.2221\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1475 - acc: 0.2680 - val_loss: 0.5183 - val_acc: 0.2223\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1442 - acc: 0.2690 - val_loss: 0.5296 - val_acc: 0.2217\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1422 - acc: 0.2695 - val_loss: 0.5248 - val_acc: 0.2232\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1402 - acc: 0.2701 - val_loss: 0.5295 - val_acc: 0.2225\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 154s 3s/step - loss: 0.1374 - acc: 0.2708 - val_loss: 0.5320 - val_acc: 0.2227\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1348 - acc: 0.2717 - val_loss: 0.5361 - val_acc: 0.2224\n",
      "Epoch 81/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1327 - acc: 0.2724 - val_loss: 0.5413 - val_acc: 0.2218\n",
      "Epoch 82/100\n",
      "55/55 [==============================] - 153s 3s/step - loss: 0.1297 - acc: 0.2733 - val_loss: 0.5467 - val_acc: 0.2217\n",
      "Epoch 83/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1281 - acc: 0.2737 - val_loss: 0.5502 - val_acc: 0.2205\n",
      "Epoch 84/100\n",
      "55/55 [==============================] - 152s 3s/step - loss: 0.1261 - acc: 0.2742 - val_loss: 0.5512 - val_acc: 0.2218\n",
      "Epoch 85/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.1259 - acc: 0.2740 - val_loss: 0.5547 - val_acc: 0.2215\n",
      "Epoch 86/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.1221 - acc: 0.2752 - val_loss: 0.5603 - val_acc: 0.2212\n",
      "Epoch 87/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.1200 - acc: 0.2762 - val_loss: 0.5613 - val_acc: 0.2216\n",
      "Epoch 88/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.1176 - acc: 0.2768 - val_loss: 0.5675 - val_acc: 0.2204\n",
      "Epoch 89/100\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.1157 - acc: 0.2774 - val_loss: 0.5692 - val_acc: 0.2212\n",
      "Epoch 90/100\n",
      "55/55 [==============================] - 27095s 493s/step - loss: 0.1145 - acc: 0.2775 - val_loss: 0.5732 - val_acc: 0.2209\n",
      "Epoch 91/100\n",
      "55/55 [==============================] - 46s 836ms/step - loss: 0.1127 - acc: 0.2785 - val_loss: 0.5759 - val_acc: 0.2207\n",
      "Epoch 92/100\n",
      "55/55 [==============================] - 44s 798ms/step - loss: 0.1102 - acc: 0.2789 - val_loss: 0.5823 - val_acc: 0.2206\n",
      "Epoch 93/100\n",
      "55/55 [==============================] - 45s 811ms/step - loss: 0.1083 - acc: 0.2795 - val_loss: 0.5816 - val_acc: 0.2204\n",
      "Epoch 94/100\n",
      "55/55 [==============================] - 54s 980ms/step - loss: 0.1065 - acc: 0.2801 - val_loss: 0.5897 - val_acc: 0.2202\n",
      "Epoch 95/100\n",
      "55/55 [==============================] - 53s 956ms/step - loss: 0.1050 - acc: 0.2806 - val_loss: 0.5914 - val_acc: 0.2204\n",
      "Epoch 96/100\n",
      "55/55 [==============================] - 48s 872ms/step - loss: 0.1033 - acc: 0.2810 - val_loss: 0.5946 - val_acc: 0.2203\n",
      "Epoch 97/100\n",
      "55/55 [==============================] - 47s 861ms/step - loss: 0.1025 - acc: 0.2811 - val_loss: 0.5990 - val_acc: 0.2204\n",
      "Epoch 98/100\n",
      "55/55 [==============================] - 66s 1s/step - loss: 0.0995 - acc: 0.2821 - val_loss: 0.6021 - val_acc: 0.2195\n",
      "Epoch 99/100\n",
      "55/55 [==============================] - 65s 1s/step - loss: 0.0989 - acc: 0.2821 - val_loss: 0.6048 - val_acc: 0.2199\n",
      "Epoch 100/100\n",
      "55/55 [==============================] - 53s 956ms/step - loss: 0.0976 - acc: 0.2827 - val_loss: 0.6101 - val_acc: 0.2198\n",
      "INFO:tensorflow:Assets written to: s2s.h6\\assets\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.3)\n",
    "model.save('s2s.h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.223548024892807\n",
      "Training Accuracy: 0.2826876640319824\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy:\", max(history1.history['val_acc']))\n",
    "print(\"Training Accuracy:\", max(history1.history['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in ip_tokens.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in op_tokens.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, op_tokens['\\t']] = 1.\n",
    "    #target_seq[0, 0, op_tokens[' ']] = 1.\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Number: 0\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Appele !\n",
      "\n",
      "Sentence Number: 1\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 2\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 3\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "Sentence Number: 4\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "Sentence Number: 5\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui l'a comment !\n",
      "\n",
      "Sentence Number: 6\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Attaquoi !\n",
      "\n",
      "Sentence Number: 7\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "Sentence Number: 8\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "Sentence Number: 9\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "Sentence Number: 10\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "Sentence Number: 11\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "Sentence Number: 12\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "Sentence Number: 13\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "Sentence Number: 14\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "Sentence Number: 15\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "Sentence Number: 16\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "Sentence Number: 17\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "Sentence Number: 18\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 19\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 20\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je vois Tom.\n",
      "\n",
      "Sentence Number: 21\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je l'ai enterté.\n",
      "\n",
      "Sentence Number: 22\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "Sentence Number: 23\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "Sentence Number: 24\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je me suis renuse.\n",
      "\n",
      "Sentence Number: 25\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Buis ça !\n",
      "\n",
      "Sentence Number: 26\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "Sentence Number: 27\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "Sentence Number: 28\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tbandille !\n",
      "\n",
      "Sentence Number: 29\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tbandille !\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for seq_index in range(30):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    number = str(seq_index)\n",
    "    print('Sentence Number: '+ number)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "1. The Seq2seq Model built using the Character-based model gives a Validation Accuracy of 22%.\n",
    "* The model predicts most of the single words correctly.\n",
    "* However, the model seems to struggle with successive words in small sentences.\n",
    "2. This is not a very acceptable Model Accuracy.\n",
    "3. In the next part, we will build the Seq2seq model using word-based tokenization and validate the overall validation data accuracy and compare the two models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
